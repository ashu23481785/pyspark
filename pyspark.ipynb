{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDKSM9Mz1l73spzpL9y5r2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashu23481785/pyspark/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Dm-wa2HVtYws",
        "outputId": "b705d905-efcf-485f-9909-eb73fdc861d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "from pyspark.sql.functions import col, lit, when, concat, date_format, to_date, unix_timestamp\n",
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "dVRFJjOYtkxG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------TO 10 Customer spent-----------------------\n",
        "inputData = sc.textFile(\"/content/customerorders.csv\")\n",
        "mappedInput = inputData.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[2])))\n",
        "aggData = mappedInput.reduceByKey(lambda x,y : x+y)\n",
        "sortedData = aggData.sortBy(lambda x: x[1],False)\n",
        "for customer, total in sortedData.take(10):\n",
        "  print(f\"Customer {customer}: {total:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6C4ix0WvWJK",
        "outputId": "4f66da7c-55a5-4ad7-b83b-b4daae8b6c86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer 68: 6375.45\n",
            "Customer 73: 6206.20\n",
            "Customer 39: 6193.11\n",
            "Customer 54: 6065.39\n",
            "Customer 71: 5995.66\n",
            "Customer 2: 5994.59\n",
            "Customer 97: 5977.19\n",
            "Customer 46: 5963.11\n",
            "Customer 42: 5696.84\n",
            "Customer 59: 5642.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------AVG MOVIE RATING-----------------------\n",
        "mvdata = sc.textFile(\"/content/moviedata.data\")\n",
        "spltData = mvdata.flatMap(lambda x: [(x.split(\"\\t\")[0],float(x.split(\"\\t\")[2]))])\n",
        "#spltData.collect()\n",
        "aggData = spltData.map(lambda x: (x[0],(x[1],1)))\n",
        "aggData1=aggData.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
        "#aggData1.take(10)\n",
        "avgRating = aggData1.mapValues(lambda x: round((x[0]/x[1]),1))\n",
        "#avgRating.take(10)\n",
        "sorData = avgRating.sortBy(lambda x: x[1],False)\n",
        "sorData.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fjtIAOJzzqh",
        "outputId": "fb4ef376-626e-405c-fe4c-731f86466fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('849', 4.9),\n",
              " ('688', 4.8),\n",
              " ('118', 4.7),\n",
              " ('507', 4.7),\n",
              " ('628', 4.7),\n",
              " ('928', 4.7),\n",
              " ('907', 4.6),\n",
              " ('686', 4.6),\n",
              " ('225', 4.5),\n",
              " ('242', 4.5)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------RATING COUNT-----------------------\n",
        "mvdata = sc.textFile(\"/content/moviedata.data\")\n",
        "spltData = mvdata.map(lambda x: x.split(\"\\t\")[2])\n",
        "#spltData.take(10)\n",
        "aggData = spltData.map(lambda x: (x,1))\n",
        "#aggData.countByValue() -- use thi directly will give result as countbyvalues are action\n",
        "#result.collect()\n",
        "#aggData.take(10)\n",
        "reducedData = aggData.reduceByKey(lambda x,y: x+y)\n",
        "sortedData = reducedData.sortBy(lambda x: x[1],False)\n",
        "sortedData.take(10)\n",
        "#for rating,count in sortedData.collect():\n",
        " # print(f\"{rating}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbKWKfAS5Z37",
        "outputId": "15f6fabc-e5d6-4333-f0df-5c478c310096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('4', 34174), ('3', 27145), ('5', 21201), ('2', 11370), ('1', 6110)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------AVG No of linkedin connectiones for each age-----------------------\n",
        "from math import *\n",
        "frndsData = sc.textFile(\"/content/friendsdata.csv\")\n",
        "def parseline(line):\n",
        "  fields = line.split(\",\")\n",
        "  age = int(fields[2])\n",
        "  numConn = int(fields[3])\n",
        "  return (age,numConn)\n",
        "spltRows = frndsData.map(parseline)\n",
        "#spltRows.take(10)\n",
        "mappedRdd = spltRows.map(lambda x:(int(x[0]),(int(x[1]),1)))\n",
        "aggData = mappedRdd.reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\n",
        "#aggData.take(10)\n",
        "avgConn = aggData.mapValues(lambda x: floor(x[0]/x[1]))\n",
        "sortData = avgConn.sortBy(lambda x: x[1],False)\n",
        "sortData.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vktF_sIZ4-_j",
        "outputId": "3aea8912-ff60-49c1-a8f0-24d030df69b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(63, 384),\n",
              " (21, 350),\n",
              " (18, 343),\n",
              " (52, 340),\n",
              " (33, 325),\n",
              " (45, 309),\n",
              " (56, 306),\n",
              " (42, 303),\n",
              " (51, 302),\n",
              " (65, 298)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Given an Input data set with name, age and city if age > 18\n",
        "#add a new column that’s populated with ‘Y’ else ‘N\n",
        "input_data = sc.textFile('/content/dataset1.text')\n",
        "input_data.take(10)\n",
        "def adultCheck(line):\n",
        "  fields = line.split(\",\")\n",
        "  name = fields[0]\n",
        "  age = int(fields[1])\n",
        "  city = fields[2]\n",
        "  adult = 'Y' if age > 18 else 'N'\n",
        "  return (name,age,city,adult)\n",
        "mapped_date = input_data.map(adultCheck)\n",
        "mapped_date.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIRjdNCa_4r6",
        "outputId": "e4622e14-2637-4c81-a9c0-8aeead00d365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sumit', 30, 'bangalore', 'Y'),\n",
              " ('kapil', 32, 'hyderabad', 'Y'),\n",
              " ('sathish', 16, 'chennai', 'N'),\n",
              " ('ravi', 39, 'bangalore', 'Y'),\n",
              " ('kavita', 12, 'hyderabad', 'N'),\n",
              " ('kavya', 19, 'mysore', 'Y')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Given the input file where columns are stationId, timeOfTheReading, readingType,\n",
        "#temperatureRecorded, and few other columns...\n",
        "#We need to find the minimum temperature for each station id.\n",
        "#Please do it using apache spark.\n",
        "\n",
        "temp_data = sc.textFile(\"/content/tempdata.csv\")\n",
        "def parse_line(line):\n",
        "  fields = line.split(',')\n",
        "  stationId = fields[0]\n",
        "  entryType = fields[2]\n",
        "  temperature = int(fields[3])\n",
        "  return (stationId, entryType, temperature)\n",
        "\n",
        "mappedInp = temp_data.map(parse_line)\n",
        "minTemp = mappedInp.filter(lambda x: x[1]== 'TMIN')\n",
        "stationId = minTemp.map(lambda x: (x[0],x[2]))\n",
        "#stationId.take(10)\n",
        "aggData = stationId.reduceByKey(lambda x,y: min(x,y))\n",
        "for station,temp in aggData.collect():\n",
        "  print(f\"{station} --> Min TMIN: {temp/10.0}°C\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVnOiXz6PmSS",
        "outputId": "23367aa3-937f-4ae0-d01e-006a1897580c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITE00100554 --> Min TMIN: -14.8°C\n",
            "EZE00100082 --> Min TMIN: -13.5°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#********Total spent on search word Data*******#\n",
        "campaign_data = sc.textFile(\"/content/bigdatacampaigndata.csv\")\n",
        "def parseline1(line):\n",
        "  fields = line.split(\",\")\n",
        "  searched_word = fields[0]\n",
        "  total_cost = fields[10]\n",
        "  if \"data\" in searched_word.lower():\n",
        "    return(\"data\",float(total_cost))\n",
        "  else:\n",
        "    return None\n",
        "mapped_data = campaign_data.map(parseline1).filter(lambda x: x is not None)\n",
        "aggData = mapped_data.reduceByKey(lambda x,y: x+y)\n",
        "for word,cost in aggData.collect():\n",
        "  print(f\"total clicked charges for word '{word.upper()}': {cost:.2f}/-\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjpjmT2jQeg5",
        "outputId": "99565893-4ce7-4471-8a6e-eb8ff955b60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total clicked charges for word 'DATA': 17987.55/-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#***********uses of broadcast variable***************************\n",
        "campaign_data = sc.textFile(\"/content/bigdatacampaigndata.csv\")\n",
        "def boringWords():\n",
        "  with open(\"/content/boringwords.text\", \"r\") as file:\n",
        "        lines = [line.strip() for line in file if line.strip()]\n",
        "        boring_words = set(lines)\n",
        "        return boring_words\n",
        "#nameSet = sc.broadcast(boringWords)\n",
        "boring_words_set = boringWords() # Execute the function on the driver\n",
        "nameSet = sc.broadcast(boring_words_set) # Broadcast the resulting set\n",
        "LoadedWrds = campaign_data.map(lambda x: (x.split(\" \")[0],round(float(x.split(\",\")[10]),2)))\n",
        "#LoadedWrds.take(10)\n",
        "aggData = LoadedWrds.reduceByKey(lambda x,y: x+y).filter(lambda x: x[0]  not in nameSet.value).map(lambda x: (x[0],round(x[1],2)))\n",
        "#aggData.take(10\n",
        "aggData.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv1_kczYQaDM",
        "outputId": "f72d0c77-f14f-497b-aaf5-da2499a34cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('big', 7990.32),\n",
              " ('hadoop', 2328.45),\n",
              " ('rdd', 19.98),\n",
              " ('leanr', 17.9),\n",
              " ('acadgild', 23.75),\n",
              " ('contentsbto', 24.66),\n",
              " ('bigdatauniversity,Broad', 24.48),\n",
              " ('intellipaat', 274.28),\n",
              " ('hive', 49.15),\n",
              " ('msc', 69.57),\n",
              " ('udacity', 69.73),\n",
              " ('4', 26.24),\n",
              " ('ivy', 64.72),\n",
              " ('iit', 82.64),\n",
              " ('scd', 24.82),\n",
              " ('birla', 29.83),\n",
              " ('deta', 49.54),\n",
              " ('learnbay', 29.81),\n",
              " ('eureka', 27.04),\n",
              " ('excelr', 24.92),\n",
              " ('intelipat,Broad', 24.25),\n",
              " ('cloudx', 52.34),\n",
              " ('datafalir,Broad', 29.78),\n",
              " ('bigdataphysicsclasses,Broad', 63.29),\n",
              " ('gcp', 22.96),\n",
              " ('cca', 62.61),\n",
              " ('edupristine', 30.0),\n",
              " ('prwatech,Broad', 24.76),\n",
              " ('https', 58.79),\n",
              " ('intellepat,Broad', 24.98),\n",
              " ('analytics', 88.59),\n",
              " ('prwatech', 24.06),\n",
              " ('trending', 21.78),\n",
              " ('simplilearn', 158.76),\n",
              " ('intellipaat,Broad', 1704.43),\n",
              " ('greatlearning', 24.05),\n",
              " ('nural', 34.22),\n",
              " ('qlikview', 29.96),\n",
              " ('cassandra', 19.81),\n",
              " ('edulab', 29.73),\n",
              " ('cloudxlab,Broad', 430.21),\n",
              " ('curso', 198.04),\n",
              " ('aws', 33.53),\n",
              " ('penerapan', 20.0),\n",
              " ('sumit', 0.58),\n",
              " ('cipet', 29.04),\n",
              " ('máster', 29.78),\n",
              " ('hadoop,Broad', 20.0),\n",
              " ('emcdsa', 24.85),\n",
              " ('rahul', 29.95),\n",
              " ('keyskills', 20.87),\n",
              " ('iguru', 58.19),\n",
              " ('acadgild,Broad', 119.43),\n",
              " ('bigdatacoursespro,Broad', 29.28),\n",
              " ('hdpcd', 20.6),\n",
              " ('intellepaat,Broad', 21.33),\n",
              " ('mitx', 24.72),\n",
              " ('trendytech', 25.89),\n",
              " ('prerequisites', 24.83),\n",
              " ('inspecs', 34.82),\n",
              " ('hiw', 34.84),\n",
              " ('intellipat,Broad', 54.97),\n",
              " ('jigsaw', 24.92),\n",
              " ('dataframe', 19.64),\n",
              " ('amity', 22.35),\n",
              " ('azure', 24.9),\n",
              " ('spark', 544.29),\n",
              " ('coursera', 103.86),\n",
              " ('ameerpet', 62.09),\n",
              " ('data', 1983.48),\n",
              " ('udemy', 54.92),\n",
              " ('congitive', 29.36),\n",
              " ('nlp', 28.15),\n",
              " ('hyderabad', 21.5),\n",
              " ('edureka', 225.35),\n",
              " ('bigdata', 199.48),\n",
              " ('_', 20.0),\n",
              " ('unacademy', 55.79),\n",
              " ('cloudera', 258.06),\n",
              " ('inceptez,Broad', 55.5),\n",
              " ('cloudxlab', 277.31),\n",
              " ('1ywar', 24.7),\n",
              " ('cca175', 29.75),\n",
              " ('1', 24.42),\n",
              " ('bihar', 25.12),\n",
              " ('axtria', 29.99),\n",
              " ('npntraining,Broad', 31.4),\n",
              " ('cognative', 34.59),\n",
              " ('dataflair,Broad', 332.55),\n",
              " ('hue', 19.21),\n",
              " ('gemology', 10.37),\n",
              " ('bangalore', 27.92),\n",
              " ('intelipatt', 67.12),\n",
              " ('technogeeks', 29.67),\n",
              " ('sqoop', 24.36),\n",
              " ('nit', 26.43),\n",
              " ('hortonworks', 24.65),\n",
              " ('congnitive', 23.58),\n",
              " ('kya', 29.17),\n",
              " ('inceptez', 21.8),\n",
              " ('vtu', 24.95),\n",
              " ('paras', 24.67),\n",
              " ('nmims', 29.84),\n",
              " ('edureka,Broad', 23.64),\n",
              " ('mapreduce', 34.99),\n",
              " ('zaloni', 29.45),\n",
              " ('itversity', 26.22),\n",
              " ('acadglid,Broad', 34.06),\n",
              " ('docker', 64.94),\n",
              " ('mongo', 27.83),\n",
              " ('cognativeclasses,Broad', 29.12),\n",
              " ('lms', 34.69),\n",
              " ('dataflair', 276.97),\n",
              " ('scala', 71.61),\n",
              " ('2', 24.29),\n",
              " ('taming', 29.2),\n",
              " ('databricks', 49.31),\n",
              " ('sujets', 19.84),\n",
              " ('kursus', 49.87),\n",
              " ('data,Broad', 4.12),\n",
              " ('wbt', 29.86),\n",
              " ('spark,Broad', 1.17),\n",
              " ('certificacion', 30.0),\n",
              " ('cursos', 58.99),\n",
              " ('cdac', 25.58),\n",
              " ('cours', 29.84),\n",
              " ('pelatihan', 15.0),\n",
              " ('étude', 29.03),\n",
              " ('analytixlabs,Broad', 27.32),\n",
              " ('hardoop', 49.56),\n",
              " ('geoinsyssoft,Broad', 28.96),\n",
              " ('mapr', 23.25),\n",
              " ('edvancer,Broad', 34.35),\n",
              " ('12', 32.94),\n",
              " ('ivyproschool,Broad', 32.35),\n",
              " ('javatpoint', 29.4)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#**********Example use of ACCUMULATOR********************\n",
        "fileinput3 = sc.textFile(\"/content/samplefile.text\")\n",
        "#fileinput3.collect()\n",
        "accum=sc.accumulator(0)\n",
        "fileinput3.foreach(lambda x: accum.add(1) if x.strip()==\"\" else None)\n",
        "print(f\"Blank lines: {accum.value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqQvJrp1AFoI",
        "outputId": "0ab06f39-2f94-4cf1-f9d4-c04cbb176ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blank lines: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "#******READ FROM LIST AND COVERT LIST TO RDD***********\n",
        "lst = [\"WARN: Tuesday 4 September 0405\",\n",
        "           \"ERROR: Tuesday 4 September 0408\",\n",
        "           \"FATAL: Wednesday 5 September 1632\",\n",
        "           \"ERROR: Friday 7 September 1854\",\n",
        "           \"FATAL: Wednesday 5 September 1632\",\n",
        "            \"ERROR: Tuesday 4 September 0408\",\n",
        "            \"ERROR: Tuesday 4 September 0408\"]\n",
        "logs_rdd = sc.parallelize(lst)\n",
        "print(logs_rdd.getNumPartitions()) #getnumpartition depends upon sc.defaultParallelism\n",
        "print(logs_rdd.defaultParallelism)\n",
        "mappedData = logs_rdd.map(lambda x: (x.split(\":\")[0],1))\n",
        "TrnsfrmData = mappedData.reduceByKey(lambda x,y:x+y)\n",
        "TrnsfrmData.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Uy_pCwvtNHmI",
        "outputId": "30a4d94b-e17f-4463-d948-6f6b01b6e27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RDD' object has no attribute 'defaultParallelism'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-791852805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlogs_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#getnumpartition depends upon sc.defaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmappedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mTrnsfrmData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'defaultParallelism'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use of GROUPBYKEY vs REDUCEBYKEY\n",
        "rf = sc.textFile('/content/bigLogNew.txt')\n",
        "spltfl = rf.map(lambda x: x.split(':')[0])\n",
        "prfl = spltfl.map(lambda x: (x,1))\n",
        "#GROUPBYKEY#\n",
        "#aggdt = prfl.groupByKey().mapValues(len) #took around 2-3 min for result\n",
        "#display(aggdt.collect())\n",
        "\n",
        "#REDUCEBYKEY#\n",
        "aggdt = prfl.reduceByKey(lambda x,y:x+y)\n",
        "aggdt.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3xRFT1ffGqs",
        "outputId": "41d27381-c308-4337-a05f-015f8a198966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('WARN', 19995544), ('ERROR', 20004456)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Amnt spent by each customer\n",
        "inptdata = sc.textFile('/content/customerorders.csv')\n",
        "def parseLine(line):\n",
        "  fields = line.split(',')\n",
        "  custId = fields[0]\n",
        "  amountspent = float(fields[2])\n",
        "  return (custId,amountspent)\n",
        "\n",
        "mappedData = inptdata.map(parseLine)\n",
        "aggData = mappedData.reduceByKey(lambda x,y: x+y)\n",
        "filteredData = aggData.filter(lambda x: x[1]>=5000)\n",
        "sortedData11 = filteredData.sortBy(lambda x: x[1],False) # Corrected index to 1 for amount\n",
        "for customer,total in sortedData11.collect():\n",
        "  print(f'Customer_id {customer} : total_spent {total:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn8P8ERHQuv4",
        "outputId": "cd5b7f88-99a5-4ea7-9319-eba81b36f29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer_id 68 : total_spent 6375.45\n",
            "Customer_id 73 : total_spent 6206.20\n",
            "Customer_id 39 : total_spent 6193.11\n",
            "Customer_id 54 : total_spent 6065.39\n",
            "Customer_id 71 : total_spent 5995.66\n",
            "Customer_id 2 : total_spent 5994.59\n",
            "Customer_id 97 : total_spent 5977.19\n",
            "Customer_id 46 : total_spent 5963.11\n",
            "Customer_id 42 : total_spent 5696.84\n",
            "Customer_id 59 : total_spent 5642.89\n",
            "Customer_id 41 : total_spent 5637.62\n",
            "Customer_id 0 : total_spent 5524.95\n",
            "Customer_id 8 : total_spent 5517.24\n",
            "Customer_id 85 : total_spent 5503.43\n",
            "Customer_id 61 : total_spent 5497.48\n",
            "Customer_id 32 : total_spent 5496.05\n",
            "Customer_id 58 : total_spent 5437.73\n",
            "Customer_id 63 : total_spent 5415.15\n",
            "Customer_id 15 : total_spent 5413.51\n",
            "Customer_id 6 : total_spent 5397.88\n",
            "Customer_id 92 : total_spent 5379.28\n",
            "Customer_id 43 : total_spent 5368.83\n",
            "Customer_id 70 : total_spent 5368.25\n",
            "Customer_id 72 : total_spent 5337.44\n",
            "Customer_id 34 : total_spent 5330.80\n",
            "Customer_id 9 : total_spent 5322.65\n",
            "Customer_id 55 : total_spent 5298.09\n",
            "Customer_id 90 : total_spent 5290.41\n",
            "Customer_id 64 : total_spent 5288.69\n",
            "Customer_id 93 : total_spent 5265.75\n",
            "Customer_id 24 : total_spent 5259.92\n",
            "Customer_id 33 : total_spent 5254.66\n",
            "Customer_id 62 : total_spent 5253.32\n",
            "Customer_id 26 : total_spent 5250.40\n",
            "Customer_id 52 : total_spent 5245.06\n",
            "Customer_id 87 : total_spent 5206.40\n",
            "Customer_id 40 : total_spent 5186.43\n",
            "Customer_id 35 : total_spent 5155.42\n",
            "Customer_id 11 : total_spent 5152.29\n",
            "Customer_id 65 : total_spent 5140.35\n",
            "Customer_id 69 : total_spent 5123.01\n",
            "Customer_id 81 : total_spent 5112.71\n",
            "Customer_id 19 : total_spent 5059.43\n",
            "Customer_id 25 : total_spent 5057.61\n",
            "Customer_id 60 : total_spent 5040.71\n",
            "Customer_id 17 : total_spent 5032.68\n",
            "Customer_id 29 : total_spent 5032.53\n",
            "Customer_id 22 : total_spent 5019.45\n",
            "Customer_id 28 : total_spent 5000.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***`PRACTICE SESSIONS BELOW`***"
      ],
      "metadata": {
        "id": "pF_vnaRDh5gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Big Data Campaign\n",
        "inpfile=sc.textFile('/content/bigdatacampaigndata.csv')\n",
        "splt_data = inpfile.map(lambda x: (float(x.split(\",\")[10]),x.split(\",\")[0].upper()))\n",
        "splt_data.take(10)\n",
        "flt_data = splt_data.flatMapValues(lambda x: x.split(\" \"))\n",
        "#flt_data.take(10)\n",
        "swp_data = flt_data.map(lambda x: (x[1],x[0]))\n",
        "agg_data=swp_data.reduceByKey(lambda x,y: x+y).sortBy(lambda x: x[1],False).filter(lambda x: x[1]>=5000)\n",
        "for word,amt in agg_data.collect():\n",
        "  print(f'Most Searched keyword \"{word}\" : Total Amount Spent \"{amt:.2f}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFT_UrHS2wr",
        "outputId": "5928b60d-d653-4bd8-e176-8e59db6fadc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Searched keyword \"DATA\" : Total Amount Spent \"16394.64\"\n",
            "Most Searched keyword \"BIG\" : Total Amount Spent \"12889.28\"\n",
            "Most Searched keyword \"IN\" : Total Amount Spent \"5774.84\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gen Auth Key as per below req\n",
        "#want to generate an auth key in Spark (PySpark), with these rules:\n",
        "#Total length = 10 bytes\n",
        "#First 4 bytes → numbers (0–9)\n",
        "#Next 4 bytes → special characters (like @#$%&*!)\n",
        "#Last 2 bytes → alphabet characters (A–Z / a–z)\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "import random\n",
        "import string\n",
        "\n",
        "def gen_auth_key():\n",
        "  digits = ''.join(random.choices(string.digits,k=4))\n",
        "  special_char=''.join(random.choices(\"!@#$%^&*()-_=+\",k=4))\n",
        "  chardata = ''.join(random.choices(string.ascii_letters,k=2))\n",
        "  return digits + special_char + chardata\n",
        "\n",
        "auth_key_udf = udf(gen_auth_key,StringType())\n",
        "\n",
        "print(gen_auth_key())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iprxJWdsuMiY",
        "outputId": "f8eb9365-bb2c-493f-f243-ae5989335999"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7032!+&*Uz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "data = [Row(1, \"John\", 30, \"Sales\", 50000.0),\n",
        " Row(2, \"Alice\", 28, \"Marketing\", 60000.0),\n",
        " Row(3, \"Bob\", 32, \"Finance\", 55000.0),\n",
        " Row(4, \"Sarah\", 29, \"Sales\", 52000.0),\n",
        " Row(5, \"Mike\", 31, \"Finance\", 58000.0)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\",IntegerType(),nullable=False),\n",
        "    StructField(\"name\",StringType(),nullable=False),\n",
        "    StructField(\"age\",IntegerType(),nullable=False),\n",
        "    StructField(\"dept\",StringType(),nullable=False),\n",
        "    StructField(\"salary\",DoubleType(),nullable=False)\n",
        "])\n",
        "\n",
        "empDf = spark.createDataFrame(data,schema)\n",
        "empDf.show()\n",
        "empDf.createOrReplaceTempView(\"emp\")\n",
        "spark.sql(\"select dept,sum(salary) as total_salary from emp group by dept\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlP2Vt-cRIfD",
        "outputId": "7010b5e6-f67a-4ff7-ce05-ba126c45a289"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+---------+-------+\n",
            "| id| name|age|     dept| salary|\n",
            "+---+-----+---+---------+-------+\n",
            "|  1| John| 30|    Sales|50000.0|\n",
            "|  2|Alice| 28|Marketing|60000.0|\n",
            "|  3|  Bob| 32|  Finance|55000.0|\n",
            "|  4|Sarah| 29|    Sales|52000.0|\n",
            "|  5| Mike| 31|  Finance|58000.0|\n",
            "+---+-----+---+---------+-------+\n",
            "\n",
            "+---------+------------+\n",
            "|     dept|total_salary|\n",
            "+---------+------------+\n",
            "|    Sales|    102000.0|\n",
            "|Marketing|     60000.0|\n",
            "|  Finance|    113000.0|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}